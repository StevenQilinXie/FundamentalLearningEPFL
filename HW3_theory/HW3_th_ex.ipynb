{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3: Unsupervised Learning\n",
    "\n",
    "# Part 1: Classification with K-means algorithm\n",
    "\n",
    "The K-means algorithm is a fundamental tool among the unsupervised learning model. Consider a problem with a dataset $\\mathcal{D} = \\left\\{x_i\\right\\}_{i=1}^n$ where $x_i \\in \\mathbb{R}^d$ with no labels, we are aiming at finding some hidden structure within the data, namely, we would like to find clusters in the dataset. Classifiers have been studied in TP but mainly for supervised learning, here the data are not labeled. \n",
    "\n",
    "The K-means algorithm tries to classify the dataset in $K$ clusters. Each cluster is represented by a centroid, meaning the average of the points within the cluster. We note $C_k$ the set of points of a cluster $k$ and $\\mu_k$ its centroid. Then, the algorithm minimizes the intra-cluster variance, in other words, it tries to reduce the distance between the points of the cluster and the centroid. \n",
    "\n",
    "More technically, the algorithm works iteratively in two main steps: \n",
    " - Points are assigned to clusters based on their proximity to existing centroid\n",
    " - Centroids are updated by taking the average of the points assigned to each cluster.\n",
    "\n",
    "\n",
    "\n",
    "The Loss function of the problem can be written as:\n",
    "$$J(\\mu_1, ..., \\mu_K) = \\sum_{i=1}^{n} \\, \\lVert x_i - \\mu(i)\\rVert^2 \\; ,$$\n",
    "where $\\mu(i)$ is the centroid of the cluster assignated to $x_i$. \n",
    "\n",
    "We want to check the understanding of the choice of this loss function. Does it match the aforementioned rules? Then, could it help to understand if the algorithm converges?\n",
    "\n",
    "Additionnally, until Question 8, we assume that the clusters $C_k$ are disjoint, especially at initialization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1:\n",
    "\n",
    "First, let us familiarize ourselves with the loss function:\n",
    "\n",
    " - Prove the second equality:\n",
    "\n",
    "   $$J(\\mu_1, ..., \\mu_K) = \\sum_{i=1}^{n} \\, \\lVert x_i - \\mu(i)\\rVert^2 = \\sum_{k=1}^{K} \\sum_{x_i \\in C_k} \\, \\lVert x_i - \\mu_k \\rVert^2$$\n",
    "\n",
    " - What does the term $\\sum_{x_i \\in C_k} \\, \\lVert x_i - \\mu_k \\rVert^2$ represent?\n",
    " \n",
    " - Explain why this form of the loss function is more convenient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answers:\n",
    "\n",
    "Each data point $x_i$ belongs to exactly one cluster $C_k$ with centroid $\\mu_k$. We can group the sum over points by clusters, then group all the clusters. This gives:\n",
    "$$\n",
    "J(\\mu_1,\\dots,\\mu_K)\n",
    "= \\sum_{i=1}^n \\|x_i - \\mu(i)\\|^2\n",
    "= \\sum_{k=1}^K \\sum_{x_i \\in C_k} \\|x_i - \\mu_k\\|^2.\n",
    "$$\n",
    "\n",
    "The term $\\sum_{x_i \\in C_k} \\, \\lVert x_i - \\mu_k \\rVert^2$ represents the intra-cluster variance, measuring how compact cluster $C_k$ is around its centroid.\n",
    "\n",
    "This cluster-wise formulation separates the loss by clusters, allows independent optimization of each centroid, and directly yields the centroid update as the mean of each cluster.\n",
    "\n",
    "$$\n",
    "\\blacksquare\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2:\n",
    "\n",
    "Let us focus on the first point of the algorihtm, let us consider a single point $x_i$ and add the time dependency. Also, we denote by a $'$ the variables after the new assignement of the data points. \n",
    "\n",
    "So that, the variables are denoted by: $(\\cdot)^t \\to (\\cdot)^t\\,{}' \\to (\\cdot)^{t+1} \\to (\\cdot)^{t+1}\\,{}' \\to (\\cdot)^{t+2}$\n",
    "\n",
    "\n",
    "Thus, at each step time $t$, the new assignements of the variables leads to: (no proof required)\n",
    "\n",
    "$$ \\mu^t(i)' = {\\rm argmin}_{\\mu \\in \\left\\{\\mu^t_k\\right\\}_k} \\lVert x_i - \\mu \\rVert^2$$\n",
    "\n",
    "For instance, at time $t$ before new assignements, the vector $x_i$ belongs to a cluster $k$, while after the next assignement, it now belongs to the cluster $k'$ (it can be the same or different from the cluster $k$). \n",
    "\n",
    "\n",
    "Thus, for the whole dataset, the algorithm updates the assignement as: $\\left\\{\\mu^t(i) \\right\\} \\to \\left\\{\\mu^t(i)'\\right\\}$.\n",
    "\n",
    "\n",
    "Compare $\\lVert x_i - \\mu^t(i) \\rVert^2$ and $\\lVert x_i - \\mu^t(i)' \\rVert^2$ for a given point. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answers:\n",
    "\n",
    "By definition of the argmin, this choice of $\\mu^t(i)'$ minimizes the squared distance to $x_i$ among all centroids. Therefore, we have\n",
    "$$\n",
    "\\|x_i - \\mu^t(i)'\\|^2 \\le \\|x_i - \\mu^t(i)\\|^2.\n",
    "$$\n",
    "Hence, the reassignment step can only decrease (or leave unchanged) the distance between $x_i$ and its assigned centroid.\n",
    "\n",
    "$$\n",
    "\\blacksquare\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3:\n",
    "\n",
    "We recall that we denote by a $'$ the variables after the new assignement of the data points, so that: $\\mu^t(i) \\to \\mu^t(i)'$ and $J_t \\to J_t'$\n",
    "\n",
    "Thanks to the previous question, compare $J_t$ and ${J_t}'$. \n",
    "\n",
    "Hint: Pick the right formula between the two given for $J$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer\n",
    "\n",
    "After the reassignment step at time $t$, the assignments change from $\\mu^t(i)$ to $\\mu^t(i)'$, while the centroids are kept fixed. Using the point-wise form of the loss,\n",
    "$$\n",
    "J_t = \\sum_{i=1}^n \\|x_i - \\mu^t(i)\\|^2,\n",
    "\\quad\n",
    "J_t' = \\sum_{i=1}^n \\|x_i - \\mu^t(i)'\\|^2.\n",
    "$$\n",
    "\n",
    "From Question 2, for each data point $x_i$,\n",
    "$$\n",
    "\\|x_i - \\mu^t(i)'\\|^2 \\le \\|x_i - \\mu^t(i)\\|^2.\n",
    "$$\n",
    "Summing this inequality over all points yields\n",
    "$$\n",
    "J_t' \\le J_t.\n",
    "$$\n",
    "\n",
    "Therefore, the reassignment step of the K-means algorithm does not increase the loss.\n",
    "\n",
    "$$\n",
    "\\blacksquare\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can go ahead by studying the second point of the algorithm. It wants to minimize the intra-cluster variance:\n",
    "\n",
    "$$ \\left\\{\\mu^{t+1}_k\\right\\}_k = {\\rm argmin}_{\\left\\{\\mu_k\\right\\}_k \\in \\mathbb{R}^d} J'_t\\left( \\left\\{\\mu_k\\right\\}_k \\right) = {\\rm argmin}_{\\left\\{\\mu_k\\right\\}_k \\in \\mathbb{R}^d} \\sum_{k=1}^{K} \\sum_{x_i \\in C^t_k{}'} \\, \\lVert x_i - \\mu_k \\rVert^2 $$\n",
    "\n",
    "\n",
    "Show that the optimization can be done cluster-wise:\n",
    "\n",
    "$$\\mu^{t+1}_k = {\\rm argmin}_{\\mu_k \\in \\mathbb{R}^d}  \\sum_{x_i \\in C^t_k{}'} \\, \\lVert x_i - \\mu_k \\rVert^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "After reassignment, the loss is\n",
    "$$\n",
    "J_t'(\\{\\mu_k\\}_{k=1}^K)=\\sum_{k=1}^K \\sum_{x_i\\in C_k^{t'}} \\|x_i-\\mu_k\\|^2.\n",
    "$$\n",
    "Define for each cluster\n",
    "$$\n",
    "f_k(\\mu_k)=\\sum_{x_i\\in C_k^{t'}} \\|x_i-\\mu_k\\|^2.\n",
    "$$\n",
    "Then\n",
    "$$\n",
    "J_t'(\\{\\mu_k\\}_{k=1}^K)=\\sum_{k=1}^K f_k(\\mu_k),\n",
    "$$\n",
    "and each term $f_k(\\mu_k)$ depends only on $\\mu_k$ (independent, no coupling between different $\\mu_j$, $j\\neq k$). Therefore the global minimization separates:\n",
    "$$\n",
    "\\arg\\min_{\\{\\mu_k\\}_{k=1}^K} J_t'(\\{\\mu_k\\}_{k=1}^K)\n",
    "=\n",
    "\\Bigl(\\arg\\min_{\\mu_1} f_1(\\mu_1),\\dots,\\arg\\min_{\\mu_K} f_K(\\mu_K)\\Bigr).\n",
    "$$\n",
    "Equivalently, for each $k$,\n",
    "$$\n",
    "\\mu_k^{t+1}=\\arg\\min_{\\mu_k\\in\\mathbb{R}^d}\\ \\sum_{x_i\\in C_k^{t'}} \\|x_i-\\mu_k\\|^2,\n",
    "$$\n",
    "so the optimization can be done cluster-wise.\n",
    "\n",
    "$$\n",
    "\\blacksquare\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Question 5\n",
    "\n",
    "Show that the new centroids of time $t+1$ are computed according to the following equality:\n",
    "\n",
    "$$ \\mu_k^{t+1} = \\frac{1}{|C^t_k{}'|}\\sum_{x_i \\in C^t_k{}'} x_i $$\n",
    "\n",
    "Does it correspond to what you expected from the algorithm? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "From Question 4, for a fixed cluster $C_k^{t'}$, the centroid update is obtained by minimizing\n",
    "$$\n",
    "\\sum_{x_i \\in C_k^{t'}} \\|x_i - \\mu_k\\|^2\n",
    "\\quad \\text{with respect to } \\mu_k.\n",
    "$$\n",
    "This is a convex quadratic function in $\\mu_k$. We can take the gradient and setting it to zero:\n",
    "$$\n",
    "-2 \\sum_{x_i \\in C_k^{t'}} (x_i - \\mu_k) = 0\n",
    "\\quad \\Rightarrow \\quad\n",
    "\\mu_k = \\frac{1}{|C_k^{t'}|} \\sum_{x_i \\in C_k^{t'}} x_i.\n",
    "$$\n",
    "Therefore,\n",
    "$$\n",
    "\\mu_k^{t+1} = \\frac{1}{|C_k^{t'}|} \\sum_{x_i \\in C_k^{t'}} x_i.\n",
    "$$\n",
    "\n",
    "This corresponds exactly to the expected behavior of the K-means algorithm, where each centroid is updated as the mean of the points assigned to its cluster.\n",
    "\n",
    "$$\n",
    "\\blacksquare\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 6:\n",
    "\n",
    "If we focus on a cluster $k$ at time $t$ after the assignement, noted $C^t_k {}'$, could you compare $\\sum_{x_i \\in C^t_k{}'} \\, \\lVert x_i - \\mu_k^t \\rVert^2$ and $\\sum_{x_i \\in C^t_k{}'} \\, \\lVert x_i - \\mu_k^{t+1} \\rVert^2$ ?\n",
    "\n",
    "What can you say about ${J_t}'$ and $J_{t+1}$ ? Hint: Use the right formula between the two given for $J$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "We know from Question 4, for a fixed cluster $C_k^{t'}$, the centroid $\\mu_k^{t+1}$ is defined as\n",
    "$$\n",
    "\\mu_k^{t+1} = \\arg\\min_{\\mu_k \\in \\mathbb{R}^d} \\sum_{x_i \\in C_k^{t'}} \\|x_i - \\mu_k\\|^2.\n",
    "$$\n",
    "Therefore, by optimality of $\\mu_k^{t+1}$,\n",
    "$$\n",
    "\\sum_{x_i \\in C_k^{t'}} \\|x_i - \\mu_k^{t+1}\\|^2\n",
    "\\;\\le\\;\n",
    "\\sum_{x_i \\in C_k^{t'}} \\|x_i - \\mu_k^{t}\\|^2.\n",
    "$$\n",
    "\n",
    "Then, by using the cluster-wise form of the loss,\n",
    "$$\n",
    "J_t' = \\sum_{k=1}^K \\sum_{x_i \\in C_k^{t'}} \\|x_i - \\mu_k^{t}\\|^2,\n",
    "\\quad\n",
    "J_{t+1} = \\sum_{k=1}^K \\sum_{x_i \\in C_k^{t'}} \\|x_i - \\mu_k^{t+1}\\|^2,\n",
    "$$\n",
    "and summing the inequality over all clusters gives\n",
    "$$\n",
    "J_{t+1} \\le J_t'.\n",
    "$$\n",
    "\n",
    "Thus, the centroid update step of K-means does not increase the loss.\n",
    "\n",
    "$$\n",
    "\\blacksquare\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 7:\n",
    "\n",
    "Putting together the Questions 3 and 5, compare $J_t$ and $J_{t+1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "From Question 3, the reassignment step satisfies\n",
    "$$\n",
    "J_t' \\le J_t.\n",
    "$$\n",
    "From Question 6, the centroid update step satisfies\n",
    "$$\n",
    "J_{t+1} \\le J_t'.\n",
    "$$\n",
    "Combining both inequalities yields\n",
    "$$\n",
    "J_{t+1} \\le J_t.\n",
    "$$\n",
    "\n",
    "Therefore, the K-means algorithm produces a non-increasing sequence of loss values, meaning that the objective function decreases or stays constant at each iteration.\n",
    "\n",
    "\n",
    "$$\n",
    "\\blacksquare\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 8:\n",
    "\n",
    "After recalling a trivial lower bound for the sequence $(J_t)_{t \\geq 0}$, what can you say about the convergence?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "From Question 7, the sequence of loss values $(J_t)_t$ is non-increasing:\n",
    "$$\n",
    "J_{t+1} \\le J_t.\n",
    "$$\n",
    "Moreover, the loss is always non-negative, since it is a sum of squared distances:\n",
    "$$\n",
    "J_t \\ge 0 \\quad \\text{for all } t \\ge 0.\n",
    "$$\n",
    "Therefore, $(J_t)_{t \\geq 0}$ is monotonically decreasing and low-bounded sequence. It must converge.\n",
    "\n",
    "$$\n",
    "\\blacksquare\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Question 9:\n",
    "\n",
    "We just proved that the algorithm converges, but what about its stability:\n",
    "\n",
    "Let us suppose that the data are sampled from a mixture of $K$ Gaussian, where the choice of $K$ is free for this question. Do you imagine a situation where the algorithm does not classify the data at all? Please design and explain the situation as clearly as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "For example, consider the case where data is sampled from a mixture of two highly overlapping Gaussian distributions, with identical means but different covariances. Even if $K=2$, the true clusters are not well separated in Euclidean distance. Since K-means relies solely on distance to centroids and assumes spherical, equally sized clusters, it may arbitrarily split the data or converge to degenerate solutions where clusters do not correspond to the underlying Gaussians.\n",
    "\n",
    "Another example is that when the Gaussians are well separated but K is poorly initialized. If both initial centroids are placed in the same Gaussian, K-means may converge to a local minimum where one Gaussian is split into two clusters and the other is ignored.\n",
    "\n",
    "In these cases, the algorithm converges but does not meaningfully classify the data, illustrating that K-means lacks stability.\n",
    "\n",
    "$$\n",
    "\\blacksquare\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 10:\n",
    "\n",
    "What can you say about those configurations of centroids? What does it imply concerning the minima? Conclude your arguments by discussing the convexity of the problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "The configurations of centroids reached by K-means correspond to fixed points of the algorithm, where neither the assignments nor the centroids change. These configurations are generally local minima ( of the K-means objective function.\n",
    "\n",
    "Since the loss function is not jointly convex in the cluster assignments and the centroids(because they are discrete), the optimization problem is non-convex. As a result, multiple local minima can exist, and the solution found by K-means depends strongly on the initialization of the centroids.\n",
    "\n",
    "This implies that K-means is guaranteed to converge, but not necessarily to the global minimum.Different initializations can lead to different final clusterings andmay converge to suboptimal or degenerate solutions.\n",
    "\n",
    "$$\n",
    "\\blacksquare\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 11:\n",
    "\n",
    "We can also quickly generalize our algorithm.\n",
    "\n",
    "In some situation, you are aiming at favoring some directions in your data and penalizing the others, so that you can weigh the euclidean distance according to:\n",
    "\n",
    "$$d^{(w)}(x_{i}, \\mu(i)) = \\frac{\\sum_{j=1}^d w_i(x_{ij} - \\mu(i)_j)^2}{\\sum_{j=1}^d w_j} $$\n",
    "\n",
    "Show that with a change of variables, the problem remains the same.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "The denominator $\\sum_{j=1}^d w_j$ is a positive constant, it does not affect the minimization and can be ignored.\n",
    "\n",
    "Define a change of variables:\n",
    "$$\n",
    "\\tilde{x}_{ij} = \\sqrt{w_i}\\, x_{ij}, \\qquad \\tilde{\\mu}(i)_{j} = \\sqrt{w_i}\\, \\mu(i)_j.\n",
    "$$\n",
    "Then,\n",
    "$$\n",
    "\\sum_{j=1}^d w_i (x_{ij}-\\mu(i)_j)^2\n",
    "= \\sum_{j=1}^d (\\tilde{x}_{ij}-\\tilde{\\mu}(i)_{j})^2\n",
    "= \\|\\tilde{x}_i-\\tilde{\\mu}(i)\\|^2.\n",
    "$$\n",
    "\n",
    "Thus, the weighted K-means objective in the original variables is equivalent to the standard K-means objective applied to the transformed data $\\tilde{x}_i$. The problem remains the same up to a linear change of variables.\n",
    "\n",
    "$$\n",
    "\\blacksquare\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Restricted Boltzmann Machine\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "The Boltzmann Machine have been inspired by thermodynamic and statistical physics models, more precisely they are part of the Energy Models using the well known Boltzmann Distribution as written in physics style:\n",
    "\n",
    "$$ P\\left( E \\right)  = \\frac{1}{Z} \\exp \\left( -\\frac{E}{k_b T} \\right)$$\n",
    "\n",
    "It becomes in statistical inference framework:\n",
    "$$\n",
    "P(\\mathbf{v} | J, \\mathbf{b}) \\propto e^{\\mathbf{v}^TJ\\mathbf{v} + \\mathbf{b}^T\\mathbf{v}} = e^{-E(\\mathbf{v})}\n",
    "$$\n",
    "where:\n",
    "- $\\mathbf{v}\\in\\mathbb{R}^n:$ The binary vector with components $v_i = 0 \\; {\\rm or} \\; 1$\n",
    "\n",
    "- $J \\in \\mathbb{R}^{n \\times n}:$ The coupling matrix\n",
    "\n",
    "- $\\mathbf{b} \\in  \\mathbb{R}^n$: Field\n",
    "\n",
    "- $E(\\mathbf{v}) \\in  \\mathbb{R}$: Energy\n",
    "\n",
    "\n",
    "However, one problem arised with initial Boltzmann Machine (BM) -- like its parent models in statistical physics (as the SK model) -- all the units are interacting through complicated dependencies. For example, if we consider 3 components of $\\mathbf{v}$: $v_1$, $v_2$, and $v_3$, there are trivial interactions such as one modelised by $P(v_1, v_2)$ corresponding to the correlation between the two first components of $\\mathbf{v}$, but there are also none trivial interactions. Indeed, if some term like $P(v_1, v_2 | v_3)$ which suggests that the correlation $x_1$ and $v_2$ depends on $v_3$ and this is clearly none linear.\n",
    "\n",
    "A really ingenious way to overcome this situation is to replace all the tricky interactions between the units $\\mathbf{v}\\in\\mathbb{R}^n$ by connections through hidden units $\\mathbf{h}\\in\\mathbb{R}^m$, artifically created. Indeed, correlations between two units $v_1$ and $v_2$ (specially the dependency of their correlations on other units $v_3$, $v_4$,...) can be atrificially replaced by introducing a third unit $h_1$ and considerin only linear correlations between $v_1 \\leftrightarrow h_1$, $h_1 \\leftrightarrow v_2$ and $v_1 \\leftrightarrow v_2$. The units $v_i$ are now called the visible units. This model is the most known version of BMs. \n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"boltzmannmachine.png\" alt=\"Diagram here\" />\n",
    "</div>\n",
    "\n",
    "\n",
    "However, this model is still fully connected and makes the computation really costful. Then, one can even simplify the model by considering zero intra layer interractions. This simplified model is call Restricted Boltzmann Machine (RBM) (Physics Nobel Price 2024 ðŸ¥³).\n",
    "\n",
    "Thus, the RBM architecture consists of two layers of binary stochastic units: a $\\textbf{visible layer}$ $\\mathbf{v}$ and a $\\textbf{hidden layer}$ $\\mathbf{h}$. The layers are fully connected, but there are no connections within a layer, making the model a $\\textbf{bipartite graph}$. \n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"rbm.png\" alt=\"Diagram here\" />\n",
    "</div>\n",
    "\n",
    "Restricted Boltzmann Machines (RBMs) are a class of energy-based probabilistic graphical models that are commonly used in machine learning for tasks such as dimensionality reduction, feature learning, and generative modeling.\n",
    "\n",
    "### Energy Function and Probabilities\n",
    "\n",
    "The joint configuration of the visible units $\\mathbf{v} \\in \\{0, 1\\}^d$ and the hidden units $\\mathbf{h} \\in \\{0, 1\\}^m$ is associated with an $\\textbf{energy function}$, defined as:\n",
    "\n",
    "$$ E(\\mathbf{v}, \\mathbf{h}) = -\\mathbf{v}^\\top \\mathbf{W} \\mathbf{h} - \\mathbf{b}^\\top \\mathbf{v} - \\mathbf{c}^\\top \\mathbf{h}$$\n",
    "where:\n",
    "- $\\mathbf{W} \\in \\mathbb{R}^{d \\times m}$ is the weight matrix connecting the visible and hidden units,\n",
    "- $\\mathbf{b} \\in \\mathbb{R}^d$ field of the visible units or also called the biases of the visible units,\n",
    "- $\\mathbf{c} \\in \\mathbb{R}^m$ field of the hidden units of also called the biases of the hidden units.\n",
    "\n",
    "The energy function determines the joint probability distribution over $\\mathbf{v}$ and $\\mathbf{h}$:\n",
    "$$ P(\\mathbf{v}, \\mathbf{h}) = \\frac{1}{Z} \\exp(-E(\\mathbf{v}, \\mathbf{h})) $$\n",
    "where $Z$ is the $\\textbf{partition function}$, ensuring normalization:\n",
    "\n",
    "$$ Z = \\sum_{\\mathbf{v}, \\mathbf{h}} \\exp(-E(\\mathbf{v}, \\mathbf{h})) $$\n",
    "\n",
    "\n",
    "The marginal probability of the visible units $\\mathbf{v}$ is obtained by summing over all possible configurations of the hidden units:\n",
    "\n",
    "$$ P(\\mathbf{v}) = \\frac{1}{Z} \\sum_{\\mathbf{h}} \\exp(-E(\\mathbf{v}, \\mathbf{h})). $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 12:\n",
    "\n",
    "Write a valid expression of the energy $E(\\textbf{v}, \\textbf{h})$ in the case of a BM (non-restricted) with an hidden layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "In a (non-restricted) Boltzmann Machine with visible units $v$ and hidden units $h$, the energy function includes interactions within and across layers. A valid expression is:\n",
    "$$\n",
    "E(v,h)\n",
    "= -\\frac{1}{2} v^\\top J_{vv} v\n",
    "  -\\frac{1}{2} h^\\top J_{hh} h\n",
    "  - v^\\top W h\n",
    "  - b^\\top v\n",
    "  - c^\\top h,\n",
    "$$\n",
    "where:\n",
    "- $J_{vv}$ models visibleâ€“visible interactions,\n",
    "- $J_{hh}$ models hiddenâ€“hidden interactions,\n",
    "- $W$ models visibleâ€“hidden interactions,\n",
    "- $b$ and $c$ are bias vectors.\n",
    "\n",
    "RBM will be the case where $J_{vv}=0$ and $J_{hh}=0$.\n",
    "\n",
    "$$\n",
    "\\blacksquare\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Independence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 13:\n",
    "\n",
    "One of the key properties of RBMs is the $\\textbf{conditional independence}$ between units within a layer:\n",
    "\n",
    "Compute the conditional probability and show that:\n",
    "\n",
    "$$ P(h_j = 1 | \\mathbf{v}) = \\sigma\\left(c_j + \\sum_{i} v_i W_{ij}\\right) $$\n",
    "and\n",
    "$$ P(v_i = 1 | \\mathbf{h}) = \\sigma\\left(b_i + \\sum_{j} h_j W_{ij}\\right) $$\n",
    "\n",
    "where $\\sigma(x) = \\frac{1}{1 + e^{-x}}$ is the sigmoid activation function.\n",
    "\n",
    "This bipartite structure enables efficient Gibbs sampling for approximating the intractable joint distribution $P(\\mathbf{v}, \\mathbf{h})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "The joint distribution of an RBM is\n",
    "$$\n",
    "P(v,h) \\propto \\exp\\!\\big(v^\\top W h + b^\\top v + c^\\top h\\big).\n",
    "$$\n",
    "\n",
    "The terms involving $h_j$ in the exponent are\n",
    "$$\n",
    "h_j\\left(c_j + \\sum_i v_i W_{ij}\\right).\n",
    "$$\n",
    "Since there are no hiddenâ€“hidden interactions, the conditional distribution factorizes:\n",
    "$$\n",
    "P(h \\mid v) = \\prod_j P(h_j \\mid v).\n",
    "$$\n",
    "For a binary hidden unit $h_j \\in \\{0,1\\}$,\n",
    "$$\n",
    "P(h_j = 1 \\mid v)\n",
    "= \\frac{\\exp\\!\\left(c_j + \\sum_i v_i W_{ij}\\right)}\n",
    "{1 + \\exp\\!\\left(c_j + \\sum_i v_i W_{ij}\\right)}\n",
    "= \\sigma\\!\\left(c_j + \\sum_i v_i W_{ij}\\right).\n",
    "$$\n",
    "\n",
    "\n",
    "Similarly, the terms involving $v_i$ are\n",
    "$$\n",
    "v_i\\left(b_i + \\sum_j h_j W_{ij}\\right),\n",
    "$$\n",
    "and since there are no visibleâ€“visible interactions,\n",
    "$$\n",
    "P(v \\mid h) = \\prod_i P(v_i \\mid h).\n",
    "$$\n",
    "Thus,\n",
    "$$\n",
    "P(v_i = 1 \\mid h)\n",
    "= \\sigma\\!\\left(b_i + \\sum_j h_j W_{ij}\\right).\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\blacksquare\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning in RBMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 14:\n",
    "\n",
    "Training an RBM involves maximizing the likelihood of the data distribution. To do so we are aiming at using a gradient descent/ascent on the weights (and biases).\n",
    "\n",
    "Compute the log-likelihood $\\mathcal{L}(\\mathbf{v})$, remember that the model is part of the unsupervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    " \n",
    "The marginal probability of a data point $v$ under the RBM is\n",
    "$$\n",
    "P(v)=\\sum_h P(v,h)=\\frac{1}{Z}\\sum_h \\exp\\!\\big(-E(v,h)\\big).\n",
    "$$\n",
    "Therefore, the log-likelihood is\n",
    "$$\n",
    "\\mathcal{L}(v)=\\log P(v)\n",
    "= \\log \\sum_h \\exp\\!\\big(-E(v,h)\\big) - \\log Z.\n",
    "$$\n",
    "For a dataset $\\{v^{(n)}\\}_{n=1}^N$, the total log-likelihood is\n",
    "$$\n",
    "\\sum_{n=1}^N \\mathcal{L}(v^{(n)}).\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\blacksquare\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 15:\n",
    "\n",
    "Compute the gradient of the log-likelihood with respect to the weights $\\mathbf{W}$ and the biases $\\mathbf{b}$, $\\mathbf{c}$ : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Question 14, the log-likelihood for one observed data point $v$ is\n",
    "$$\n",
    "\\mathcal{L}(v)=\\log \\sum_h e^{-E(v,h)}-\\log Z.\n",
    "$$\n",
    "\n",
    "Using the RBM energy function\n",
    "$$\n",
    "E(v,h)=-v^\\top W h - b^\\top v - c^\\top h,\n",
    "$$\n",
    "the partial derivative of the energy with respect to a weight $W_{ij}$ is\n",
    "$$\n",
    "\\frac{\\partial E(v,h)}{\\partial W_{ij}} = - v_i h_j.\n",
    "$$\n",
    "\n",
    "Consider the first term:\n",
    "$$\n",
    "\\log \\sum_h e^{-E(v,h)}.\n",
    "$$\n",
    "Applying the chain rule,\n",
    "$$\n",
    "\\frac{\\partial}{\\partial W_{ij}} \\log \\sum_h e^{-E(v,h)}\n",
    "= \\frac{1}{\\sum_h e^{-E(v,h)}} \\sum_h e^{-E(v,h)}\n",
    "\\left(-\\frac{\\partial E(v,h)}{\\partial W_{ij}}\\right).\n",
    "$$\n",
    "\n",
    "Substituting $\\frac{\\partial E(v,h)}{\\partial W_{ij}} = -v_i h_j$ gives\n",
    "$$\n",
    "= \\sum_h \\frac{e^{-E(v,h)}}{\\sum_{h'} e^{-E(v,h')}} \\, v_i h_j.\n",
    "$$\n",
    "\n",
    "The fraction is exactly the conditional probability $P(h\\mid v)$. Hence,\n",
    "$$\n",
    "= \\sum_h P(h\\mid v)\\, v_i h_j\n",
    "= \\mathbb{E}_{P(h\\mid v)}[v_i h_j].\n",
    "$$\n",
    "\n",
    "Similarly, differentiate the second term $-\\log Z$:\n",
    "$$\n",
    "\\frac{\\partial}{\\partial W_{ij}} \\log Z\n",
    "= \\sum_{v,h} \\frac{e^{-E(v,h)}}{Z} \\, v_i h_j\n",
    "= \\mathbb{E}_{P(v,h)}[v_i h_j].\n",
    "$$\n",
    "\n",
    "Combining both parts yields\n",
    "$$\n",
    "\\boxed{\n",
    "\\frac{\\partial \\mathcal{L}(v)}{\\partial W_{ij}}\n",
    "= \\mathbb{E}_{P(h\\mid v)}[v_i h_j]\n",
    "- \\mathbb{E}_{P(v,h)}[v_i h_j].\n",
    "}\n",
    "$$\n",
    "\n",
    "\n",
    "Now we look at gradient with respect to $b$. Since,\n",
    "$$\n",
    "\\frac{\\partial E(v,h)}{\\partial b_i} = -v_i,\n",
    "$$\n",
    "the same differentiation steps apply as above, leading to\n",
    "$$\n",
    "\\boxed{\n",
    "\\frac{\\partial \\mathcal{L}(v)}{\\partial b_i}\n",
    "= \\mathbb{E}_{P(h\\mid v)}[v_i]\n",
    "- \\mathbb{E}_{P(v,h)}[v_i].\n",
    "}\n",
    "$$\n",
    "\n",
    "Now, the gradient with respect to $c$. Similarly, since\n",
    "$$\n",
    "\\frac{\\partial E(v,h)}{\\partial c_j} = -h_j,\n",
    "$$\n",
    "we obtain\n",
    "$$\n",
    "\\boxed{\n",
    "\\frac{\\partial \\mathcal{L}(v)}{\\partial c_j}\n",
    "= \\mathbb{E}_{P(h\\mid v)}[h_j]\n",
    "- \\mathbb{E}_{P(v,h)}[h_j].\n",
    "}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\blacksquare\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it should be possible to implement the RBM!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 16: (Open question)\n",
    "\n",
    "While it seems possible to run RBM algorithm, note that the second term in the gradient w.r.t. $\\mathbf{W}$ is computationally expensive due to the intractability of $Z$, the approximation Contrastive Divergence - k is often use. Research what is this approximation, is this approximation enough, why? Explain it with your own words and cite the papers you used for your documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "Hinton (2002) proposed Contrastive Divergence (CD-k) as a practical approximation. Instead of sampling from the true model distribution, CD-k uses a short Gibbs sampling chain initialized at the data. Starting from a real data point, the algorithm alternates between sampling hidden and visible units for $k$ steps, and the final sample is used to approximate the negative phase of the gradient.\n",
    "\n",
    "This approximation avoids directly computing $Z$ entirely and makes training RBMs feasible, and it works well in practice. Even using a very small number of steps (often $k=1$) is sufficient to learn useful representations. Intuitively, the early Gibbs steps already move the model distribution in a direction that reduces the mismatch with the data.\n",
    "\n",
    "In summary, this method is good enough in practice. Despite the lack of strict theoretical guarantees,its efficiency and simplicity explain why it is widely used for training RBMs, .\n",
    "\n",
    "Reference:\n",
    "\n",
    "- Hinton, G. E. (2002). *Training products of experts by minimizing contrastive divergence*. Neural Computation. https://www.cs.toronto.edu/~fritz/absps/tr00-004.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications of RBMs\n",
    "\n",
    "RBMs are widely used in tasks such as:\n",
    "\n",
    "- $\\textbf{Dimensionality reduction}$: Similar to PCA but capable of capturing non-linear structures,\n",
    "- $\\textbf{Feature learning}$: For pre-training deep neural networks,\n",
    "- $\\textbf{Collaborative filtering}$: Used in recommendation systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
