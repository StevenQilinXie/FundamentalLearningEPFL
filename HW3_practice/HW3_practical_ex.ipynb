{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3: optimization of a CNN model\n",
    "The task of this homework is to optimize a CNN model for the CIFAR-100. You are free to define the architecture of the model, and the training procedure. The only contraints are:\n",
    "- It must be a `torch.nn.Module` object\n",
    "- The number of trained parameters must be less than 1 million\n",
    "- The test dataset must not be used for any step of training.\n",
    "- The final training notebook should run on Google Colab within a maximum 1 hour approximately.\n",
    "- Do not modify the random seed, as they are needed for reproducibility purpose.\n",
    "\n",
    "For the grading, you must use the `evaluate` function defined below. It takes a model as input, and returns the test accuracy as output.\n",
    "\n",
    "As a guideline, you are expected to **discuss** and motivate your choices regarding:\n",
    "- Model architecture\n",
    "- Hyperparameters (learning rate, batch size, etc)\n",
    "- Regularization methods\n",
    "- Optimizer\n",
    "- Validation scheme\n",
    "\n",
    "A code without any explanation of the choices will not be accepted. Test accuracy is not the only measure of success for this homework.\n",
    "\n",
    "Remember that most of the train process is randomized, store your model's weights after training and load it before the evaluation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading packages and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "\n",
    "# Fix all random seeds\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# For full determinism\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Import the best device available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.mps.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "# load the data\n",
    "train_dataset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=torchvision.transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "def evaluate(model):\n",
    "    params_count = sum(p.numel() for p in model.parameters())\n",
    "    print('The model has {} parameters'.format(params_count))\n",
    "\n",
    "    if params_count > int(1e6):\n",
    "        print('The model has too many parameters! Not allowed to evaluate.')\n",
    "        return\n",
    "\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "\n",
    "    # print in bold red in a notebook\n",
    "    print('\\033[1m\\033[91mAccuracy on the test set: {}%\\033[0m'.format(100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of a simple CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters:  556708\n"
     ]
    }
   ],
   "source": [
    "class TinyNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TinyNet, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = torch.nn.Linear(8*8*64, 128)\n",
    "        self.fc2 = torch.nn.Linear(128, 100)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.relu(self.conv1(x))\n",
    "        x = torch.nn.functional.max_pool2d(x, 2)\n",
    "        x = torch.nn.functional.relu(self.conv2(x))\n",
    "        x = torch.nn.functional.max_pool2d(x, 2)\n",
    "        x = x.view(-1, 8*8*64)\n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "print(\"Model parameters: \", sum(p.numel() for p in TinyNet().parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of basic training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 4.6139\n",
      "Epoch [2/10], Loss: 4.5809\n",
      "Epoch [2/10], Loss: 4.5809\n",
      "Epoch [3/10], Loss: 4.5756\n",
      "Epoch [3/10], Loss: 4.5756\n",
      "Epoch [4/10], Loss: 4.5896\n",
      "Epoch [4/10], Loss: 4.5896\n",
      "Epoch [5/10], Loss: 4.5961\n",
      "Epoch [5/10], Loss: 4.5961\n",
      "Epoch [6/10], Loss: 4.5979\n",
      "Epoch [6/10], Loss: 4.5979\n",
      "Epoch [7/10], Loss: 4.6145\n",
      "Epoch [7/10], Loss: 4.6145\n",
      "Epoch [8/10], Loss: 4.5321\n",
      "Epoch [8/10], Loss: 4.5321\n",
      "Epoch [9/10], Loss: 4.5010\n",
      "Epoch [9/10], Loss: 4.5010\n",
      "Epoch [10/10], Loss: 4.3955\n",
      "Epoch [10/10], Loss: 4.3955\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = TinyNet()\n",
    "model.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "for epoch in range(10):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, 10, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 556708 parameters\n",
      "\u001b[1m\u001b[91mAccuracy on the test set: 3.37%\u001b[0m\n",
      "\u001b[1m\u001b[91mAccuracy on the test set: 3.37%\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# save the model on a file\n",
    "torch.save(model.state_dict(), 'tiny_net.pt')\n",
    "\n",
    "loaded_model = TinyNet()\n",
    "loaded_model.load_state_dict(torch.load('tiny_net.pt', weights_only=True))\n",
    "evaluate(loaded_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improved Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proposed Improved Training Approach\n",
    "\n",
    "- **Model Architecture:** Compact CNN under 1M params using depthwise-separable convolutions, BatchNorm, Dropout, and global average pooling. Optionally add lightweight residual connections to stabilize deeper stacks.\n",
    "- **Data Pipeline:** Normalize with CIFAR-100 stats (mean `[0.5071, 0.4867, 0.4408]`, std `[0.2675, 0.2565, 0.2761]`). Apply `RandomCrop(32, padding=4)`, `RandomHorizontalFlip`, mild `ColorJitter`, and `RandomErasing`. Consider `RandAugment` or `AutoAugment` if training time permits.\n",
    "- **Validation Scheme:** Stratified or random 90/10 split from training set, fixed by the provided seed for reproducibility. Track best validation accuracy and use early stopping (patience ~10).\n",
    "- **Optimizer & LR Schedule:** Prefer `AdamW` (lr ~1e-3, weight_decay=5e-4) or `SGD` with Nesterov (lr warmup then cosine annealing). Cosine schedule generally improves convergence versus multi-step.\n",
    "- **Regularization:** Use label smoothing (e.g., `0.1`), Dropout2d in conv blocks, gradient clipping (e.g., `1.0`). Maintain an EMA of model parameters for evaluation to reduce variance.\n",
    "- **Performance Tricks:** Mixed precision (AMP) on CUDA, `pin_memory` for DataLoaders, adequate `num_workers` (2â€“4). Save checkpoints to `small_cifar_net_best.pt` on validation improvement.\n",
    "- **Evaluation:** Ensure test loader uses the same normalization as training. Reload best weights and evaluate with the provided `evaluate(model)` function.\n",
    "\n",
    "This setup balances efficiency and accuracy within the parameter/time constraints while keeping seeds unchanged for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# CIFAR-100 normalization\n",
    "CIFAR100_MEAN = (0.5071, 0.4867, 0.4408)\n",
    "CIFAR100_STD = (0.2675, 0.2565, 0.2761)\n",
    "\n",
    "# Transforms: augmentations for train, normalization for val/test\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.02),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR100_MEAN, CIFAR100_STD),\n",
    "    transforms.RandomErasing(p=0.25)\n",
    "])\n",
    "\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR100_MEAN, CIFAR100_STD)\n",
    "])\n",
    "\n",
    "# Recreate datasets with proper transforms\n",
    "train_dataset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=train_transform)\n",
    "val_dataset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=val_test_transform)\n",
    "# Use normalized test dataset for evaluation only\n",
    "test_dataset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=val_test_transform)\n",
    "\n",
    "# Validation split (seed already set earlier)\n",
    "val_ratio = 0.1\n",
    "num_train = len(train_dataset)\n",
    "indices = np.arange(num_train)\n",
    "np.random.shuffle(indices)\n",
    "val_size = int(num_train * val_ratio)\n",
    "val_indices = indices[:val_size]\n",
    "train_indices = indices[val_size:]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "# DataLoaders (no CUDA: disable pin_memory)\n",
    "batch_size = 128\n",
    "num_workers = 2\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler, num_workers=num_workers, pin_memory=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, sampler=val_sampler, num_workers=num_workers, pin_memory=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 765055\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DSConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, stride=1, p_drop=0.1):\n",
    "        super().__init__()\n",
    "        self.depthwise = nn.Conv2d(in_ch, in_ch, kernel_size=3, stride=stride, padding=1, groups=in_ch, bias=False)\n",
    "        self.pointwise = nn.Conv2d(in_ch, out_ch, kernel_size=1, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_ch)\n",
    "        self.dropout = nn.Dropout2d(p_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.depthwise(x)\n",
    "        x = self.pointwise(x)\n",
    "        x = self.bn(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class SmallCIFARNet(nn.Module):\n",
    "    def __init__(self, num_classes=100):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            DSConvBlock(3, 64, stride=1, p_drop=0.05),\n",
    "            nn.MaxPool2d(2),  # 32 -> 16\n",
    "            DSConvBlock(64, 128, stride=1, p_drop=0.10),\n",
    "            nn.MaxPool2d(2),  # 16 -> 8\n",
    "            DSConvBlock(128, 256, stride=1, p_drop=0.15),\n",
    "            nn.MaxPool2d(2),  # 8 -> 4\n",
    "            DSConvBlock(256, 512, stride=1, p_drop=0.20),\n",
    "            # Extra capacity at 512 channels\n",
    "            DSConvBlock(512, 512, stride=1, p_drop=0.20),\n",
    "            DSConvBlock(512, 512, stride=1, p_drop=0.20),\n",
    "        )\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        self.classifier = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.gap(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "model = SmallCIFARNet().to(device)\n",
    "print('Model parameters:', sum(p.numel() for p in model.parameters()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import torch\n",
    "\n",
    "# EMA helpers\n",
    "\n",
    "def init_ema(model):\n",
    "    ema = deepcopy(model)\n",
    "    for p in ema.parameters():\n",
    "        p.requires_grad_(False)\n",
    "    return ema\n",
    "\n",
    "\n",
    "def update_ema(ema_model, model, decay):\n",
    "    with torch.no_grad():\n",
    "        msd = model.state_dict()\n",
    "        for k, v in ema_model.state_dict().items():\n",
    "            v.copy_(v * decay + msd[k] * (1.0 - decay))\n",
    "\n",
    "ema_decay = 0.995\n",
    "ema_model = init_ema(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Optimizer, scheduler, loss, and training hyperparameters\n",
    "base_lr = 1e-3\n",
    "weight_decay = 5e-4\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=base_lr, weight_decay=weight_decay)\n",
    "\n",
    "epochs = 50\n",
    "warmup_epochs = 5\n",
    "\n",
    "\n",
    "def lr_lambda(e):\n",
    "    if e < warmup_epochs:\n",
    "        return (e + 1) / warmup_epochs\n",
    "    t = (e - warmup_epochs) / max(1, epochs - warmup_epochs)\n",
    "    return 0.5 * (1 + math.cos(math.pi * t))\n",
    "\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "grad_clip = 1.0\n",
    "patience = 10\n",
    "best_val_acc = 0.0\n",
    "no_improve = 0\n",
    "best_path = 'small_cifar_net_best.pt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | loss: 4.3586 | train acc: 4.57% | val acc: 0.94% | lr: 0.00040\n",
      "Epoch 2/50 | loss: 4.0377 | train acc: 9.82% | val acc: 4.86% | lr: 0.00060\n",
      "Epoch 2/50 | loss: 4.0377 | train acc: 9.82% | val acc: 4.86% | lr: 0.00060\n",
      "Epoch 3/50 | loss: 3.8282 | train acc: 13.69% | val acc: 18.80% | lr: 0.00080\n",
      "Epoch 3/50 | loss: 3.8282 | train acc: 13.69% | val acc: 18.80% | lr: 0.00080\n",
      "Epoch 4/50 | loss: 3.6633 | train acc: 17.45% | val acc: 24.82% | lr: 0.00100\n",
      "Epoch 4/50 | loss: 3.6633 | train acc: 17.45% | val acc: 24.82% | lr: 0.00100\n",
      "Epoch 5/50 | loss: 3.5432 | train acc: 20.01% | val acc: 29.56% | lr: 0.00100\n",
      "Epoch 5/50 | loss: 3.5432 | train acc: 20.01% | val acc: 29.56% | lr: 0.00100\n",
      "Epoch 6/50 | loss: 3.4328 | train acc: 22.76% | val acc: 32.32% | lr: 0.00100\n",
      "Epoch 6/50 | loss: 3.4328 | train acc: 22.76% | val acc: 32.32% | lr: 0.00100\n",
      "Epoch 7/50 | loss: 3.3457 | train acc: 24.80% | val acc: 34.20% | lr: 0.00100\n",
      "Epoch 7/50 | loss: 3.3457 | train acc: 24.80% | val acc: 34.20% | lr: 0.00100\n",
      "Epoch 8/50 | loss: 3.2827 | train acc: 26.15% | val acc: 36.40% | lr: 0.00099\n",
      "Epoch 8/50 | loss: 3.2827 | train acc: 26.15% | val acc: 36.40% | lr: 0.00099\n",
      "Epoch 9/50 | loss: 3.2233 | train acc: 27.97% | val acc: 38.12% | lr: 0.00098\n",
      "Epoch 9/50 | loss: 3.2233 | train acc: 27.97% | val acc: 38.12% | lr: 0.00098\n",
      "Epoch 10/50 | loss: 3.1681 | train acc: 29.13% | val acc: 39.48% | lr: 0.00097\n",
      "Epoch 10/50 | loss: 3.1681 | train acc: 29.13% | val acc: 39.48% | lr: 0.00097\n",
      "Epoch 11/50 | loss: 3.1252 | train acc: 30.18% | val acc: 40.76% | lr: 0.00096\n",
      "Epoch 11/50 | loss: 3.1252 | train acc: 30.18% | val acc: 40.76% | lr: 0.00096\n",
      "Epoch 12/50 | loss: 3.0916 | train acc: 31.22% | val acc: 41.06% | lr: 0.00094\n",
      "Epoch 12/50 | loss: 3.0916 | train acc: 31.22% | val acc: 41.06% | lr: 0.00094\n",
      "Epoch 13/50 | loss: 3.0508 | train acc: 32.41% | val acc: 42.98% | lr: 0.00092\n",
      "Epoch 13/50 | loss: 3.0508 | train acc: 32.41% | val acc: 42.98% | lr: 0.00092\n",
      "Epoch 14/50 | loss: 3.0155 | train acc: 33.25% | val acc: 43.88% | lr: 0.00090\n",
      "Epoch 14/50 | loss: 3.0155 | train acc: 33.25% | val acc: 43.88% | lr: 0.00090\n",
      "Epoch 15/50 | loss: 2.9822 | train acc: 34.29% | val acc: 44.88% | lr: 0.00088\n",
      "Epoch 15/50 | loss: 2.9822 | train acc: 34.29% | val acc: 44.88% | lr: 0.00088\n",
      "Epoch 16/50 | loss: 2.9571 | train acc: 34.76% | val acc: 45.28% | lr: 0.00086\n",
      "Epoch 16/50 | loss: 2.9571 | train acc: 34.76% | val acc: 45.28% | lr: 0.00086\n",
      "Epoch 17/50 | loss: 2.9282 | train acc: 35.46% | val acc: 46.10% | lr: 0.00083\n",
      "Epoch 17/50 | loss: 2.9282 | train acc: 35.46% | val acc: 46.10% | lr: 0.00083\n",
      "Epoch 18/50 | loss: 2.9108 | train acc: 36.10% | val acc: 46.76% | lr: 0.00081\n",
      "Epoch 18/50 | loss: 2.9108 | train acc: 36.10% | val acc: 46.76% | lr: 0.00081\n",
      "Epoch 19/50 | loss: 2.8755 | train acc: 37.08% | val acc: 47.02% | lr: 0.00078\n",
      "Epoch 19/50 | loss: 2.8755 | train acc: 37.08% | val acc: 47.02% | lr: 0.00078\n",
      "Epoch 20/50 | loss: 2.8546 | train acc: 37.31% | val acc: 47.62% | lr: 0.00075\n",
      "Epoch 20/50 | loss: 2.8546 | train acc: 37.31% | val acc: 47.62% | lr: 0.00075\n",
      "Epoch 21/50 | loss: 2.8392 | train acc: 38.12% | val acc: 48.26% | lr: 0.00072\n",
      "Epoch 21/50 | loss: 2.8392 | train acc: 38.12% | val acc: 48.26% | lr: 0.00072\n",
      "Epoch 22/50 | loss: 2.8189 | train acc: 38.53% | val acc: 49.12% | lr: 0.00069\n",
      "Epoch 22/50 | loss: 2.8189 | train acc: 38.53% | val acc: 49.12% | lr: 0.00069\n",
      "Epoch 23/50 | loss: 2.7960 | train acc: 39.21% | val acc: 49.24% | lr: 0.00065\n",
      "Epoch 23/50 | loss: 2.7960 | train acc: 39.21% | val acc: 49.24% | lr: 0.00065\n",
      "Epoch 24/50 | loss: 2.7796 | train acc: 39.67% | val acc: 49.72% | lr: 0.00062\n",
      "Epoch 24/50 | loss: 2.7796 | train acc: 39.67% | val acc: 49.72% | lr: 0.00062\n",
      "Epoch 25/50 | loss: 2.7560 | train acc: 40.40% | val acc: 50.26% | lr: 0.00059\n",
      "Epoch 25/50 | loss: 2.7560 | train acc: 40.40% | val acc: 50.26% | lr: 0.00059\n",
      "Epoch 26/50 | loss: 2.7415 | train acc: 40.65% | val acc: 50.54% | lr: 0.00055\n",
      "Epoch 26/50 | loss: 2.7415 | train acc: 40.65% | val acc: 50.54% | lr: 0.00055\n",
      "Epoch 27/50 | loss: 2.7231 | train acc: 41.50% | val acc: 51.02% | lr: 0.00052\n",
      "Epoch 27/50 | loss: 2.7231 | train acc: 41.50% | val acc: 51.02% | lr: 0.00052\n",
      "Epoch 28/50 | loss: 2.7103 | train acc: 41.38% | val acc: 51.32% | lr: 0.00048\n",
      "Epoch 28/50 | loss: 2.7103 | train acc: 41.38% | val acc: 51.32% | lr: 0.00048\n",
      "Epoch 29/50 | loss: 2.6917 | train acc: 41.89% | val acc: 51.30% | lr: 0.00045\n",
      "Epoch 29/50 | loss: 2.6917 | train acc: 41.89% | val acc: 51.30% | lr: 0.00045\n",
      "Epoch 30/50 | loss: 2.6846 | train acc: 42.16% | val acc: 51.36% | lr: 0.00041\n",
      "Epoch 30/50 | loss: 2.6846 | train acc: 42.16% | val acc: 51.36% | lr: 0.00041\n",
      "Epoch 31/50 | loss: 2.6731 | train acc: 42.66% | val acc: 51.56% | lr: 0.00038\n",
      "Epoch 31/50 | loss: 2.6731 | train acc: 42.66% | val acc: 51.56% | lr: 0.00038\n",
      "Epoch 32/50 | loss: 2.6560 | train acc: 42.88% | val acc: 51.80% | lr: 0.00035\n",
      "Epoch 32/50 | loss: 2.6560 | train acc: 42.88% | val acc: 51.80% | lr: 0.00035\n",
      "Epoch 33/50 | loss: 2.6440 | train acc: 43.25% | val acc: 52.04% | lr: 0.00031\n",
      "Epoch 33/50 | loss: 2.6440 | train acc: 43.25% | val acc: 52.04% | lr: 0.00031\n",
      "Epoch 34/50 | loss: 2.6394 | train acc: 43.68% | val acc: 52.30% | lr: 0.00028\n",
      "Epoch 34/50 | loss: 2.6394 | train acc: 43.68% | val acc: 52.30% | lr: 0.00028\n",
      "Epoch 35/50 | loss: 2.6274 | train acc: 43.82% | val acc: 52.72% | lr: 0.00025\n",
      "Epoch 35/50 | loss: 2.6274 | train acc: 43.82% | val acc: 52.72% | lr: 0.00025\n",
      "Epoch 36/50 | loss: 2.6217 | train acc: 44.26% | val acc: 52.56% | lr: 0.00022\n",
      "Epoch 36/50 | loss: 2.6217 | train acc: 44.26% | val acc: 52.56% | lr: 0.00022\n",
      "Epoch 37/50 | loss: 2.6091 | train acc: 44.57% | val acc: 52.70% | lr: 0.00019\n",
      "Epoch 37/50 | loss: 2.6091 | train acc: 44.57% | val acc: 52.70% | lr: 0.00019\n",
      "Epoch 38/50 | loss: 2.6065 | train acc: 44.25% | val acc: 52.86% | lr: 0.00017\n",
      "Epoch 38/50 | loss: 2.6065 | train acc: 44.25% | val acc: 52.86% | lr: 0.00017\n",
      "Epoch 39/50 | loss: 2.5885 | train acc: 45.05% | val acc: 53.12% | lr: 0.00014\n",
      "Epoch 39/50 | loss: 2.5885 | train acc: 45.05% | val acc: 53.12% | lr: 0.00014\n",
      "Epoch 40/50 | loss: 2.5908 | train acc: 44.90% | val acc: 53.20% | lr: 0.00012\n",
      "Epoch 40/50 | loss: 2.5908 | train acc: 44.90% | val acc: 53.20% | lr: 0.00012\n",
      "Epoch 41/50 | loss: 2.5740 | train acc: 45.51% | val acc: 53.22% | lr: 0.00010\n",
      "Epoch 41/50 | loss: 2.5740 | train acc: 45.51% | val acc: 53.22% | lr: 0.00010\n",
      "Epoch 42/50 | loss: 2.5710 | train acc: 45.43% | val acc: 53.38% | lr: 0.00008\n",
      "Epoch 42/50 | loss: 2.5710 | train acc: 45.43% | val acc: 53.38% | lr: 0.00008\n",
      "Epoch 43/50 | loss: 2.5620 | train acc: 45.52% | val acc: 53.30% | lr: 0.00006\n",
      "Epoch 43/50 | loss: 2.5620 | train acc: 45.52% | val acc: 53.30% | lr: 0.00006\n",
      "Epoch 44/50 | loss: 2.5729 | train acc: 45.37% | val acc: 53.42% | lr: 0.00004\n",
      "Epoch 44/50 | loss: 2.5729 | train acc: 45.37% | val acc: 53.42% | lr: 0.00004\n",
      "Epoch 45/50 | loss: 2.5728 | train acc: 45.28% | val acc: 53.48% | lr: 0.00003\n",
      "Epoch 45/50 | loss: 2.5728 | train acc: 45.28% | val acc: 53.48% | lr: 0.00003\n",
      "Epoch 46/50 | loss: 2.5608 | train acc: 45.79% | val acc: 53.54% | lr: 0.00002\n",
      "Epoch 46/50 | loss: 2.5608 | train acc: 45.79% | val acc: 53.54% | lr: 0.00002\n",
      "Epoch 47/50 | loss: 2.5557 | train acc: 46.38% | val acc: 53.44% | lr: 0.00001\n",
      "Epoch 47/50 | loss: 2.5557 | train acc: 46.38% | val acc: 53.44% | lr: 0.00001\n",
      "Epoch 48/50 | loss: 2.5597 | train acc: 45.66% | val acc: 53.54% | lr: 0.00000\n",
      "Epoch 48/50 | loss: 2.5597 | train acc: 45.66% | val acc: 53.54% | lr: 0.00000\n",
      "Epoch 49/50 | loss: 2.5582 | train acc: 45.73% | val acc: 53.58% | lr: 0.00000\n",
      "Epoch 49/50 | loss: 2.5582 | train acc: 45.73% | val acc: 53.58% | lr: 0.00000\n",
      "Epoch 50/50 | loss: 2.5553 | train acc: 45.91% | val acc: 53.54% | lr: 0.00000\n",
      "Epoch 50/50 | loss: 2.5553 | train acc: 45.91% | val acc: 53.54% | lr: 0.00000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Training loop (no CUDA/AMP)\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        if grad_clip is not None:\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct = (preds == labels).sum().item()\n",
    "        bs = labels.size(0)\n",
    "        total += bs\n",
    "        running_loss += loss.item() * bs\n",
    "        running_acc += correct\n",
    "\n",
    "        update_ema(ema_model, model, ema_decay)\n",
    "\n",
    "    scheduler.step()\n",
    "    epoch_loss = running_loss / max(1, total)\n",
    "    epoch_acc = running_acc / max(1, total)\n",
    "\n",
    "    # Validation using EMA weights\n",
    "    ema_model.eval()\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = ema_model(images)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            val_correct += (preds == labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "    val_acc = val_correct / max(1, val_total)\n",
    "\n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
    "    print(f'Epoch {epoch+1}/{epochs} | loss: {epoch_loss:.4f} | train acc: {epoch_acc*100:.2f}% | val acc: {val_acc*100:.2f}% | lr: {current_lr:.5f}')\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        no_improve = 0\n",
    "        torch.save(ema_model.state_dict(), best_path)\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve >= patience:\n",
    "            print(f'Early stopping at epoch {epoch+1}. Best val acc: {best_val_acc*100:.2f}%')\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 765055 parameters\n",
      "\u001b[1m\u001b[91mAccuracy on the test set: 52.95%\u001b[0m\n",
      "\u001b[1m\u001b[91mAccuracy on the test set: 52.95%\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Load best and evaluate on test set\n",
    "\n",
    "eval_model = SmallCIFARNet().to(device)\n",
    "eval_model.load_state_dict(torch.load(best_path, weights_only=True, map_location=device))\n",
    "loaded_model = eval_model\n",
    "\n",
    "# Use provided evaluate(model) function\n",
    "evaluate(loaded_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
